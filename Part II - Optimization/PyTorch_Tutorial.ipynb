{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"nbbIa6w7Ifbr"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\fuwen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","from time import time\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"taGn5jj_gYD8"},"source":["An open source machine learning framework that accelerates the path from research prototyping to production deployment [PyTorch](https://pytorch.org/). To check the installation instructions check [PyTorch](https://pytorch.org/). We will import PyTorch and use it in todya's lab to implement simple and convoutional neural networks.\n"]},{"cell_type":"markdown","metadata":{"id":"qqR7Z027gheU"},"source":["Let's start by checking the version of PyTorch"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686133961523,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"YuhtOOmPXVbz","outputId":"72c060cf-2d66-4e10-e756-022426291d98"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using torch 1.13.1+cpu\n"]}],"source":["print(\"Using torch\", torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"wDWOeBIcg-I4"},"source":["In several codes you will see the manaul_seed setup of PyTorch. This is done to ensure reproducibility of the code. Run the below code multiple times with and without the manual seed setup and you will that the random number geneator produces same random numbers every time when the seed is set."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":827,"status":"ok","timestamp":1686134022614,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"k7bTTNPfXn6d","outputId":"26eb9250-242d-4aaf-effd-0f14ed1023b5"},"outputs":[{"data":{"text/plain":["tensor([0.0461, 0.4024])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["#torch.manual_seed(1234)\n","torch.randn(2)"]},{"cell_type":"markdown","metadata":{"id":"zrfQdvFiheUI"},"source":["The basic data structure for PyTorch (and for that matter most other deep learning libraries) is Tensor. The simplest definition of a Tensor is \"Tensor is a multi-dimensional matrix\".\n","\n","There are multiple ways of defining tensors in PyTorch below we will show two such methods. Run the code and see what tensors are produced. For ease of notation I have used {variable name}_t to represent tensor variables in the code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lOrY92jee9t-"},"outputs":[],"source":["# Creat Pytorch Tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1128,"status":"ok","timestamp":1686133646700,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"xKN-0kncJFnJ","outputId":"08998f0a-0824-4f1b-b0e3-13b0fe3a6785"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1., 0.],\n","        [0., 1.]])\n","torch.Size([2, 2])\n","torch.float32\n","torch.int32\n","tensor([[0.8042, 0.1620, 0.9258],\n","        [0.0756, 0.9674, 0.1743],\n","        [0.5089, 0.7478, 0.2859]], dtype=torch.float64)\n"]}],"source":["x=[[1,0],[0,1]]\n","x_t=torch.Tensor(x)\n","print(x_t)\n","print(x_t.shape)\n","\n","print(x_t.dtype)\n","x_t = x_t.int()\n","print(x_t.dtype)\n","\n","\n","x=np.random.rand(3,3)\n","x_t=torch.from_numpy(x)\n","print(x_t)"]},{"cell_type":"markdown","metadata":{"id":"fgX3E361iTnK"},"source":["Tensors in PyTorch have several useful attributes (properties). Some of these attributes are shape, requires_grad, dtype, and device. shape attributes shows the shape of the Tensor, similar to the shape of a matrix. requries_grad is a flag that shows weather the gradients w.r.t the tensor are calculated or not. If requires_grad is True it will mean that PyTorch will keep of the gradient of the [output] w.r.t. the Tensor varaiable. dtype shows the data type of the tensor. device shows the device on which the tensor resides cpu or cuda (GPU). We will discuss why this is an important feature of the Tensor and how it helps us to speed up our codes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ZgKMTfRfOlq"},"outputs":[],"source":["# Tensor as an Object"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":399,"status":"ok","timestamp":1685693812502,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"eJuke9VzHiZX","outputId":"17731695-0d38-4147-9c89-90cee54d4ea8"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["x_t.requires_grad = True\n","print(x_t.requires_grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":925,"status":"ok","timestamp":1685693818091,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"G182F45Ppi_w","outputId":"4f1946ed-8eec-48e2-e609-1eb0817a8d87"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","True\n","cpu\n","cuda:0\n"]}],"source":["print(x_t.requires_grad)\n","x_t.requires_grad = True\n","print(x_t.requires_grad)\n","print(x_t.device)\n","\n","x_t = x_t.to(\"cuda\")\n","print(x_t.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":375,"status":"ok","timestamp":1685693871935,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"wOAwhSpDJbTm","outputId":"eacc7bf9-069d-4760-ca45-2fe8b30636c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 3])\n","True\n","torch.float64\n","cuda:0\n","True\n"]}],"source":["print(x_t.shape)\n","print(x_t.requires_grad)\n","print(x_t.dtype)\n","print(x_t.device)\n","\n","x_t=torch.from_numpy(x)\n","x_t.requires_grad = True\n","\n","print(x_t.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"iBlbMTekjafO"},"source":["GPU (Graphics Processing Unit) is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles. [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit#:~:text=A%20graphics%20processing%20unit%20(GPU,%2C%20workstations%2C%20and%20game%20consoles.)\n","\n","\n","GPUs are extremely beneficial in running matrix operations and as we will most of deep learning is composed of matrix opearations, we can significantly speed up our codes by using GPUs.\n","\n","There are several methods and attributes in PyTorch to manage the device on which Tensors can reside. Below we look at a few."]},{"cell_type":"markdown","metadata":{"id":"Jr1xhDQFmYpY"},"source":["GPUs are specialized expensive piece of equipment and might not be available on all machines. To check if a GPU is available on a machine we can use torch.cuda.is_available() method which returns a True value if a GPUs is available.\n","\n","Note: To use a GPU on Google Colab you can select the option under Edit->NoteBook Settings.\n","\n","\n","To move Tensor between CPU and GPU we can use Tensor.to(...) method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7eyRVOFfXfG"},"outputs":[],"source":["# Devices: GPU or CPU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1686134313657,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"Y4C5VqV1zsti","outputId":"0073de8f-65e5-434e-97ab-87f142589e67"},"outputs":[{"name":"stdout","output_type":"stream","text":["I have a GPU in this machine\n"]}],"source":["if torch.cuda.is_available():\n","  print(\"I have a GPU in this machine\")\n","else:\n","  print(\"No GPU is available in this machine\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1132,"status":"ok","timestamp":1686134321318,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"8vY1QOC4yGlt","outputId":"a6418c50-56d4-4e75-e9a1-247a56f8432f"},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["print(torch.cuda.device_count())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1686133725795,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"lN_deHP3SUXB","outputId":"ef888232-4442-44a2-ee83-c97f43680ce8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Jun  7 10:28:44 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P8     9W /  70W |      3MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1685693912136,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"Rc7S5PmdqMME","outputId":"4b830952-3364-4cd0-a070-f31f794c92ba"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["if torch.cuda.is_available():\n","  device = torch.device(\"cuda:0\")\n","\n","else:\n","  device = torch.device(\"cpu\")\n","\n","x_t = x_t.to(device)\n","print(x_t.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":463,"status":"ok","timestamp":1685693921735,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"agUklojrydGd","outputId":"93465a4e-3d90-40ba-f5a6-e85460eb65b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["x_t = x_t.to(\"cpu\")\n","print(x_t.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":800,"status":"ok","timestamp":1685693925546,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"Z7Zdi1rr8c8V","outputId":"625041ba-150c-4405-ccc7-92d197042f42"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["print(torch.cuda.is_available())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":417,"status":"ok","timestamp":1685693928333,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"2fM5YPTc8rdQ","outputId":"0f74d009-9b49-4067-d19a-c0a54095b71c"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n","tensor([[3.5955, 3.5288, 0.9141],\n","        [4.0810, 4.3005, 4.8160],\n","        [4.0865, 2.4850, 1.5268]], device='cuda:0', dtype=torch.float64,\n","       grad_fn=<MulBackward0>)\n","tensor([[3.5955, 3.5288, 0.9141],\n","        [4.0810, 4.3005, 4.8160],\n","        [4.0865, 2.4850, 1.5268]], device='cuda:0', dtype=torch.float64,\n","       grad_fn=<MulBackward0>)\n"]}],"source":["x_t = x_t.to(\"cuda\")\n","print(x_t.device)\n","print(x_t*5)\n","\n","x_w = x_t.to(\"cuda:0\")\n","print(x_w*5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":855,"status":"ok","timestamp":1685693933175,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"tpcy8y1VLLg9","outputId":"9166ac29-39c3-40c1-8cf2-2b5d198379a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","1\n","cuda:0\n","cpu\n","cuda:0\n"]}],"source":["print(torch.cuda.is_available())\n","\n","print(torch.cuda.device_count())\n","if torch.cuda.is_available():\n","  device=torch.device(\"cuda:0\")\n","else:\n","  device=torch.device(\"cpu\")\n","\n","\n","print(device)\n","device=torch.device(\"cpu\")\n","x_t_cpu=x_t.to(device)\n","print(x_t_cpu.device)\n","device=torch.device(\"cuda\")\n","x_t_gpu=x_t.to(device)\n","print(x_t_gpu.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1685693937522,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"3Tp2wEdEdURf","outputId":"7e0dacd4-cb76-40e2-acbc-6a39aca50baa"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n","cuda:0\n"]}],"source":["device=torch.device(\"cpu\")\n","x_t_cpu=x_t.to(device)\n","print(x_t_cpu.device)\n","device=torch.device(\"cuda\")\n","x_t_gpu=x_t.to(device)\n","print(x_t_gpu.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XcnYsKVTQ_To"},"outputs":[],"source":["use_gpu=True\n","if use_gpu:\n","  device=torch.device(\"cuda:0\")\n","else:\n","  device=torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"16qjvhZ_owQ7"},"source":["We have dedicated functions in PyTorch to create tensors of particular types. Some of these are provided below. torch.zeros, creates a tensor of zeors, torch.ones, crates a tensor of ones, torch.eye creats an indentity matrix, torch.rand creats a tensor or random values sampled fron uniform distribution, torch.randn creates a tensor of random values samples from unit normal (gaussian distribution), torch.arange(N) ceates whole numbers till N-1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J1_tgKvof6pz"},"outputs":[],"source":["# Create Common Tensors"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":449,"status":"ok","timestamp":1685693943862,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"DogSSBKNzFhI","outputId":"2cb7d716-b967-47e3-d443-c80c3abdabdc"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.3904, 0.6009, 0.2566]])\n"]}],"source":["x = torch.rand((1,3))\n","print(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":393,"status":"ok","timestamp":1685693946635,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"iB8ebKmroQ7C","outputId":"19f51e0f-cf63-4246-9fd4-7c7ddff8ba31"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [0., 0., 0.]])\n","tensor([[1., 1.],\n","        [1., 1.]])\n","tensor([[1., 0.],\n","        [0., 1.]])\n","tensor([[0.7936],\n","        [0.9408]])\n","tensor([[ 1.5231,  0.6647],\n","        [-1.0324, -0.2770],\n","        [-0.1671, -0.1079]])\n","tensor([0, 1, 2, 3, 4])\n"]}],"source":["x=torch.zeros(3,3)\n","print(x)\n","x=torch.ones(2,2)\n","print(x)\n","x=torch.eye(2)\n","print(x)\n","x=torch.rand(2,1)\n","print(x)\n","x=torch.randn((3,2))\n","print(x)\n","x=torch.arange(5)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"Xzdy_333nEH3"},"source":["Tensor opeartions:\n","Most numpy operations are also available in PyTorch. opeartaions like addition (+), subtraction (-), mulitplication (*), divistion (/), matrix multiplication (@), and power(***)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2U2NRMcgGX1"},"outputs":[],"source":["# Tensor operations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1102,"status":"ok","timestamp":1685693951732,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"vaOiJ9ffLkUV","outputId":"6c3cd808-e7d8-4d46-ee61-a74eff65e63f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-1.4285, -0.2810,  0.7489],\n","        [ 1.1164,  1.2931,  0.4137],\n","        [-0.5710, -0.9749,  0.1863]]) tensor([[1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.]])\n","element wise of the sum of the two tensors istensor([[-0.4285, -0.2810,  0.7489],\n","        [ 1.1164,  2.2931,  0.4137],\n","        [-0.5710, -0.9749,  1.1863]])\n","element wise of the diff of the two tensors istensor([[-2.4285, -0.2810,  0.7489],\n","        [ 1.1164,  0.2931,  0.4137],\n","        [-0.5710, -0.9749, -0.8137]])\n","element wise of the prod of the two tensors istensor([[-1.4285, -0.0000,  0.0000],\n","        [ 0.0000,  1.2931,  0.0000],\n","        [-0.0000, -0.0000,  0.1863]])\n","element wise of the div of the two tensors istensor([[-0.7000, -0.0000,  0.0000],\n","        [ 0.0000,  0.7733,  0.0000],\n","        [-0.0000, -0.0000,  5.3664]])\n","element wise of the pow of the two tensors istensor([[-1.4285,  1.0000,  1.0000],\n","        [ 1.0000,  1.2931,  1.0000],\n","        [ 1.0000,  1.0000,  0.1863]])\n","matrix multiplicaiton of the two tensors istensor([[-1.4285, -0.2810,  0.7489],\n","        [ 1.1164,  1.2931,  0.4137],\n","        [-0.5710, -0.9749,  0.1863]])\n"]}],"source":["x=torch.randn((3,3))\n","y=torch.eye(3)\n","print(x,y)\n","\n","zsum=x+y\n","zdiff=x-y\n","zprod=x*y\n","zdiv=y/x\n","# @ is read as at\n","zmatmul=x@y\n","zpow=x**y\n","\n","print(f\"element wise of the sum of the two tensors is{zsum}\")\n","print(f\"element wise of the diff of the two tensors is{zdiff}\")\n","print(f\"element wise of the prod of the two tensors is{zprod}\")\n","print(f\"element wise of the div of the two tensors is{zdiv}\")\n","print(f\"element wise of the pow of the two tensors is{zpow}\")\n","print(f\"matrix multiplicaiton of the two tensors is{zmatmul}\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"N3Ids2kfrh8_"},"source":["We can use Tensor1 [operation]= Tensor2 for in place operations on Tensor1.\n","\n","Several PyTorch functions also have an inplace version with the function name appended with and underscore."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":677,"status":"ok","timestamp":1685693969024,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"oODYh0ExOWfB","outputId":"1b2d62ce-2900-4474-daf3-694e33bb998e"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-1.4285, -0.2810,  0.7489],\n","        [ 1.1164,  1.2931,  0.4137],\n","        [-0.5710, -0.9749,  0.1863]])\n","tensor([[-0.5000, -0.2810,  0.5000],\n","        [ 0.5000,  0.5000,  0.4137],\n","        [-0.5000, -0.5000,  0.1863]])\n"]}],"source":["#pytorch also has inplace operations\n","\n","x+=y #x=x+y\n","x-=y\n","y*=x\n","y/=x\n","#x**=y.  x=x**y\n","x@=y\n","\n","\n","x.clamp(-0.5,0.5)\n","print(x)\n","x.clamp_(-0.5,0.5)\n","print(x)"]},{"cell_type":"markdown","metadata":{"id":"T1g-q8Njr8My"},"source":["PyTorch uses same indexing and slicing conventians as numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_JwUpzfgge0h"},"outputs":[],"source":["# Indexing and slicing Tensor, just like Numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":411,"status":"ok","timestamp":1685693999395,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"2L8bo5hiQoCJ","outputId":"6bcc34d2-58af-42d4-ca5e-e59650394001"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1., 2., 3.],\n","        [4., 5., 6.],\n","        [7., 8., 9.]])\n","torch.Size([3, 3])\n","tensor(5.)\n","tensor([4., 5., 6.]) tensor([2., 5., 8.])\n","torch.Size([3]) torch.Size([3])\n","tensor([[4., 5., 6.]]) tensor([[2.],\n","        [5.],\n","        [8.]])\n","torch.Size([1, 3]) torch.Size([3, 1])\n"]}],"source":["\n","x=torch.Tensor([[1,2,3],[4,5,6],[7,8,9]])\n","print(x)\n","print(x.shape)\n","#an element can be access by specifying the row and column\n","print(x[1,1])\n","# acessing rows and columns\n","print(x[1,:], x[:,1])\n","print(x[1,:].shape, x[:,1].shape)\n","# acessing rows and columns and keeping the shape\n","print(x[1:2,:], x[:,1:2])\n","print(x[1:2,:].shape, x[:,1:2].shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":744,"status":"ok","timestamp":1685694338128,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"8KkUSKvi8Bc3","outputId":"b7d7dc40-4e86-4c10-8329-4b366bc577b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1., 2., 3.],\n","        [4., 5., 6.],\n","        [7., 8., 9.]])\n","tensor([[5., 6.],\n","        [8., 9.]])\n"]}],"source":["\n","x=torch.Tensor([[1,2,3],[4,5,6],[7,8,9]])\n","print(x)\n","\n","\n","print(x[1:,1:])\n"]},{"cell_type":"markdown","metadata":{"id":"AuEk5nq3sBcN"},"source":["Some Tensor operations in PyTorch are:\n","concateation, and reshaping."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":818,"status":"ok","timestamp":1685694353153,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"EgqHOJ9sUivV","outputId":"6b2549bd-c2d0-4532-e9d1-928b4a97d0ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1., 2., 3., 1., 0., 0.],\n","        [4., 5., 6., 0., 1., 0.],\n","        [7., 8., 9., 0., 0., 1.]])\n","tensor([[1., 2.],\n","        [3., 1.],\n","        [0., 0.],\n","        [4., 5.],\n","        [6., 0.],\n","        [1., 0.],\n","        [7., 8.],\n","        [9., 0.],\n","        [0., 1.]])\n","tensor([[1., 2.],\n","        [3., 1.],\n","        [0., 0.],\n","        [4., 5.],\n","        [6., 0.],\n","        [1., 0.],\n","        [7., 8.],\n","        [9., 0.],\n","        [0., 1.]])\n"]}],"source":["\n","z=torch.cat((x,y), dim=1)\n","print(z)\n","\n","z2=z.reshape(9,2)\n","print(z2)\n","z2=torch.reshape(z,(9,2))\n","print(z2)"]},{"cell_type":"markdown","metadata":{"id":"XCLS-tPmsXud"},"source":["removing a unit dimension through squeeze, adding a unit dimension through unsqueeze."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1685694419916,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"T9eUDOf1iLli","outputId":"4582eb29-b822-4e5a-bd5f-e16199727a2a"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 2, 3, 1, 5])\n","torch.Size([1, 1, 2, 3, 1, 5])\n","torch.Size([1, 1, 2, 3, 1, 5])\n","torch.Size([1, 1, 2, 3, 1, 5])\n","torch.Size([2, 3, 5])\n"]}],"source":["\n","x=torch.rand((1,2,3,1,5))\n","print(x.shape)\n","\n","x_u=x.unsqueeze(dim=0)\n","print(x_u.shape)\n","\n","x.unsqueeze_(dim=1)\n","print(x.shape)\n","\n","\n","\n","print(x.shape)\n","x_s=x.squeeze()\n","print(x_s.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aEtdef4Gh01r"},"outputs":[],"source":["# GPU vs CPU"]},{"cell_type":"markdown","metadata":{"id":"6twOFl_Vsju2"},"source":["Why use GPU?\n","Example of matrix multiplication on CPU and GPU.\n","\n","Important points to consdier:\n","Always use vector opeartions\n","Always use PyTorch functions\n","When possible always use GPUs for matrix operations"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24270,"status":"ok","timestamp":1685694588115,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"7tSEtyTaiSZf","outputId":"abab11bc-8da6-4d41-e4b6-58c63306eb08"},"outputs":[{"name":"stdout","output_type":"stream","text":["time taken by the nested loop multiplciation is 23.62109661102295 seconds\n","time taken by the single loop multiplciation is 0.16016221046447754 seconds\n","time taken by  multiplciation on cpu is 0.03321266174316406 seconds\n","time taken by multiplciation on GPU is 0.1290431022644043 seconds\n"]}],"source":["# use vectorized operations and broadcasting\n","# use GPU for vector/matrix operations\n","siz=1000\n","\n","x=torch.randn((siz,siz))\n","y=torch.randn((siz,siz))\n","z=torch.zeros((siz,siz))\n","st=time()\n","for i in range(siz):\n","  for j in range(siz):\n","    z[i,j]=torch.dot(x[i,:],y[:,j])\n","\n","ed=time()\n","print(f\"time taken by the nested loop multiplciation is {ed-st} seconds\")\n","st=time()\n","for i in range(siz):\n","  z[i,:]=torch.mm(x[i,:].unsqueeze(dim=0),y)\n","\n","ed=time()\n","print(f\"time taken by the single loop multiplciation is {ed-st} seconds\")\n","st=time()\n","z=torch.mm(x,y)\n","\n","ed=time()\n","print(f\"time taken by  multiplciation on cpu is {ed-st} seconds\")\n","\n","device=torch.device(\"cuda:0\")\n","x=x.to(device)\n","y=y.to(device)\n","z=z.to(device)\n","st=time()\n","z=torch.mm(x,y)\n","ed=time()\n","print(f\"time taken by multiplciation on GPU is {ed-st} seconds\")\n"]},{"cell_type":"markdown","metadata":{"id":"gjOc-p0KtSw8"},"source":["Autograd example with PyTorch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PhfuEjYViKxl"},"outputs":[],"source":["# Get derivatives automatically"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":509,"status":"ok","timestamp":1685694555701,"user":{"displayName":"Jindong Gu","userId":"02445578988487457576"},"user_tz":-60},"id":"mNO62Il31R7G","outputId":"4b3250e8-18e6-4dac-9f8d-0689f54f923f"},"outputs":[{"name":"stdout","output_type":"stream","text":["None\n","None\n","tensor([[1.6273]])\n","After backpass\n","dz/dx\n","tensor([[2.]])\n","None\n","tensor([[1.6273]])\n","tensor([10.])\n","None\n","tensor([5.])\n"]}],"source":["x=torch.randn((1,1), requires_grad=True)\n","y=torch.Tensor([5.0])\n","y.requires_grad=True\n","\n","z=2*x+y*y\n","# dz/dx = 2\n","# dz/dy = 2*y\n","\n","print(x.grad)\n","print(x.grad_fn)\n","print(x.data)\n","\n","z.backward()\n","print(\"After backpass\")\n","print(\"dz/dx\")\n","print(x.grad)\n","print(y.grad_fn)\n","print(x.data)\n","\n","\n","print(y.grad)\n","print(y.grad_fn)\n","print(y.data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ShV1OIE3sp2K","outputId":"e5c6b899-af64-46a4-9713-eef27d82c9c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["dz/dx is tensor([[2.]])\n","dx/dy is tensor([0.2837])\n"]}],"source":["x=torch.randn((1,1), requires_grad=True)\n","y=torch.Tensor([5.0])\n","y.requires_grad=True\n","\n","z=2*x+torch.sin(y)\n","z.backward()\n","print(f\"dz/dx is {x.grad}\")\n","print(f\"dx/dy is {y.grad}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEq1soIxefcZ","outputId":"e4b1ca25-5169-401e-fed7-4e3c10954e1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["After backpass\n","dz/dx\n","tensor([[2.]])\n","dz/dy\n","tensor([-0.1011])\n"]}],"source":["x=torch.randn((1,1), requires_grad=True)\n","y=torch.Tensor([5.0])\n","y.requires_grad=True\n","\n","z=2*x+y*y*torch.exp(-y)\n","\n","z.backward()\n","print(\"After backpass\")\n","print(\"dz/dx\")\n","print(x.grad)\n","print(\"dz/dy\")\n","print(y.grad)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P7EVtohYrymk","outputId":"10238d91-39c5-4329-9ad3-2f49fb5da40d"},"outputs":[{"name":"stdout","output_type":"stream","text":["None\n","None\n","tensor([[0.0106]])\n","After backpass\n","dz/dx\n","tensor([[2.]])\n","<AddBackward0 object at 0x7f0404c33c10>\n","tensor([[0.0106]])\n","dz/dy\n","tensor([10.])\n","tensor([[25.0212]], grad_fn=<AddBackward0>)\n","tensor([5.])\n","True\n","False\n","<class 'numpy.ndarray'>\n"]}],"source":["x=torch.randn((1,1), requires_grad=True)\n","y=torch.Tensor([5.0])\n","y.requires_grad=True\n","\n","z=2*x+y*y\n","# dz/dx = 2\n","# dz/dy = 2*y\n","\n","print(x.grad)\n","print(x.grad_fn)\n","print(x.data)\n","z.backward()\n","print(\"After backpass\")\n","print(\"dz/dx\")\n","print(x.grad)\n","print(z.grad_fn)\n","print(x.data)\n","\n","print(\"dz/dy\")\n","print(y.grad)\n","print(z)\n","print(y.data)\n","print(z.requires_grad)\n","with torch.no_grad():\n","  z=2*x+y*y\n","  print(z.requires_grad)\n","\n","\n","#detaching variable and coverting back to numpy\n","x=x.detach().cpu().numpy()\n","print(type(x))"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1aE1byZvbGiLru_KYJfrsqfGlLawcZ1rk","timestamp":1707510569042}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
