{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Expectations:** run some stockastic optimisers to fit a linear model on the MNIST data set of hand written digits.\n",
        "\n",
        "\n",
        "**Note we have swapped from using numpy to torch, the syntax is very similar!**"
      ],
      "metadata": {
        "id": "_-O8HVTpkbZm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vqg8SEXq55vV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models,transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GENERATE MNIST DATASET - no need to edit\n",
        "def mnist(batch_sz, valid_size=0.2, shuffle=True, random_seed=2000):\n",
        "    num_classes = 10\n",
        "    transform_train = transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                    ])\n",
        "\n",
        "    transform_valid = transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                    ])\n",
        "\n",
        "    # Training dataset\n",
        "    train_data = MNIST(root='./datasets', train=True, download=True, transform=transform_train)\n",
        "    valid_data = MNIST(root='./datasets', train=True, download=True, transform=transform_valid)\n",
        "    num_train = len(train_data)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "    if shuffle == True:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_sz, sampler=train_sampler,pin_memory=True)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_sz, sampler=valid_sampler,pin_memory=True)\n",
        "\n",
        "    # Test dataset\n",
        "    test_data = MNIST(root='./datasets', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data,\n",
        "                                              batch_size=batch_sz, shuffle=False, pin_memory=True)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader"
      ],
      "metadata": {
        "id": "BDiyOVBF6Ess"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64 # this is batch size i.e. the number of rows in a batch of data, feel free try other values {32, 64, 128, 256, 512}\n",
        "\n",
        "input_dimension = 1 * 28 * 28 # MNIST images are grey scale images 28 pixels wide and high\n",
        "d = input_dimension # if you prefer a shorter varible name\n",
        "\n",
        "number_of_classes = 10 # number of class of MNIST data set C = {0,1,2,3,4,5,6,7,8,9}\n",
        "\n",
        "train_loader, valid_loader, test_loader=mnist(batch_size) # Create the data loaders these are objects that when looping over present batchs of data\n",
        "\n",
        "# lists to store loss and accuracy values\n",
        "sgd_train_losses, sgd_val_losses, sgd_train_acc, sgd_val_acc = [], [], [], []\n",
        "opt_train_losses, opt_val_losses, opt_train_acc, opt_val_acc = [], [], [], []\n",
        "adam_train_losses, adam_val_losses, adam_train_acc, adam_val_acc = [], [], [], []\n"
      ],
      "metadata": {
        "id": "5AaDsz2E6HsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))\n",
        "batch_content = batch[0]\n",
        "print('dimension of single batch: ',batch_content.size())\n",
        "print('code will convert 28*28 image in a single 784 vector of pixel values on which we will fit a linear model')"
      ],
      "metadata": {
        "id": "LBjoQZbpAEI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ac1e65-d1d2-46b3-c4c9-e80cdcc6f5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dimension of single batch:  torch.Size([64, 1, 28, 28])\n",
            "code will convert 28*28 image in a single 784 vector of pixel values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visulise some training examples\n",
        "k = 0\n",
        "h = 10\n",
        "f, axs = plt.subplots(1,h)\n",
        "\n",
        "for k in range(h):\n",
        "  img = batch_content[k,:,:,:].squeeze()\n",
        "  axs[k].imshow(img)\n",
        "  axs[k].axis(\"off\")"
      ],
      "metadata": {
        "id": "nrKlapwHLnwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Function** for mnist digits we will use the cross entropy loss:\n",
        "\n",
        "\\begin{align*}\n",
        "\\text{Cross Entropy Loss} : \\frac{1}{N}\\sum_{z=1}^N \\ell_{z}(\\mathbf{y}_{z},\\mathbf{y}_z^*) = - \\frac{1}{N}\\sum_{z=1}^N\\sum_{c \\in C}y^*_{z,c} \\text{log}(y_{z,c}) = - \\frac{1}{N}\\sum_{z=1}^N\\mathbf{y}^*_{z} \\text{log}(\\text{softmax}(\\mathbf{x}_z \\mathbf{w}))\n",
        "\\end{align*}\n",
        "\n",
        "where $y^*_z$ is $c_z^{th}$ unit vector corrisponding were $c_z$ is the class of the $z^{th}$ training example. For a concrete example is the class of the first example was 0, $y_0^* = [1,0,0,0,0,0,0,0,0,0]$.\n",
        "\n",
        "\\begin{align*}\n",
        "\\text{softmax}(\\mathbf{x}) = \\frac{\\exp{\\mathbf{x}}}{\\sum_i \\exp{x_i}}\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "ldqlOxAC2u5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# By default the data loader gives you the class value as a integer rather than a one hot vector as show above\n",
        "\n",
        "def index_to_one_hot(y, classes=10):\n",
        "  b = len(y) # batchsize\n",
        "  one_hot = torch.zeros((b, classes), device=y.device)\n",
        "  xs = torch.arange(b, device=y.device)\n",
        "  one_hot[xs,y] = 1\n",
        "  return one_hot\n",
        "\n",
        "y = torch.randint(10,(10,))\n",
        "print('y',y)\n",
        "print('y',index_to_one_hot(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhPMnbbcO5aK",
        "outputId": "f598a810-68ac-464d-ecc3-75fdd4abeb96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y tensor([8, 8, 4, 9, 4, 5, 0, 1, 3, 1])\n",
            "y tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1** Complete the CE loss function below:"
      ],
      "metadata": {
        "id": "J61UATmsZJJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As before please complete the following two functions. Remember here X and y are batches of b examles.\n",
        "\n",
        "def CE_loss(X, ystar, w):\n",
        "  # ----------------------------\n",
        "  # your code here\n",
        "  loss = 0\n",
        "\n",
        "\n",
        "  # ---------------------------\n",
        "  return float(loss)"
      ],
      "metadata": {
        "id": "oTqQkRPCMZLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = index_to_one_hot(y)\n",
        "X = torch.randn(10,10)\n",
        "\n",
        "print('your fucntion:',float(CE_loss(X,Y,torch.eye(10).float())))\n",
        "print('should match this fucntion:',float(F.cross_entropy(X, y)))"
      ],
      "metadata": {
        "id": "NZILb_wNWSUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2** the gradeint of the CE loss is\n",
        "\n",
        "\\begin{equation}\n",
        "  \\nabla_\\ell(W) = -\\frac{1}{N} \\sum_{z=1}^N \\mathbf{x}_z ( \\mathbf{y}^*_{z} - \\text{softmax} (W^\\top\\mathbf{x}_z)^\\top)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "See if you can derivce this,\n",
        "(**Hint** use the chain rule\n",
        "$\\frac{\\partial \\ell}{\\partial \\mathbf{w}_c} = \\sum_{z=1}^N \\sum_{i=1}^C (\\frac{\\partial \\ell}{\\partial \\hat{y}_{i,z}} \\frac{\\partial \\hat{y}_{i,z}}{\\partial \\mathbf{w}_c} )\n",
        "$, try setting $\\hat{y}_{i,z} = \\text{softmax}(\\mathbf{x}_z \\mathbf{w}_i)$)\n",
        "\n"
      ],
      "metadata": {
        "id": "PE-PjbJsYeXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3** complete the function to calcululat the gradeint of the CE loss below.\n"
      ],
      "metadata": {
        "id": "pa9NVhR87ZHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_func(X, ystar, w):\n",
        "  n = X.size()[0]\n",
        "  # your code here\n",
        "  # ----------------------------\n",
        "  grad = 0\n",
        "\n",
        "\n",
        "  # ----------------------------\n",
        "  return grad"
      ],
      "metadata": {
        "id": "svSFTEhVYGC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# useful function to calculate accuracy\n",
        "\n",
        "def accuracy(out, targets):\n",
        "    _, pred = torch.max(out, 1)\n",
        "    correct = torch.eq(pred, targets).sum()\n",
        "    acc = torch.mean(torch.eq(pred, targets).float())\n",
        "    return correct, float(100. * acc)"
      ],
      "metadata": {
        "id": "rS6Ny54b1UJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(w, valid_loader): # function that evaluates loss on vaildation set\n",
        "    total_loss, total_correct, total_num = 0, 0, 0\n",
        "    for batch in valid_loader:\n",
        "      X,y = batch[0].to(device).view(batch[0].size()[0],input_dimension), batch[1].to(device)\n",
        "      Y = index_to_one_hot(y, classes=10)\n",
        "      loss_train = CE_loss(X, Y, w)\n",
        "      correct, acc = accuracy(X.mm(w), y)\n",
        "      total_correct += correct\n",
        "      total_loss += loss_train*X.size()[0]\n",
        "      total_num += X.size()[0]\n",
        "    return float(total_loss/total_num), float(total_correct/total_num * 100)"
      ],
      "metadata": {
        "id": "zBsY4v8uC9GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4** implement the SGD with momentum and experiement with with different batch sizes. What do you notice?"
      ],
      "metadata": {
        "id": "0tjm8UXs7kDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # in this exersize we will not use autograd you will have calucated gradent\n",
        "\n",
        "  # fixing a device to run the codes on\n",
        "  device = torch.device(\"cuda:0\")\n",
        "\n",
        "  w = torch.zeros(input_dimension,number_of_classes).to(device)\n",
        "  m = torch.zeros(input_dimension,number_of_classes).to(device)\n",
        "\n",
        "  eta = 1e-1\n",
        "  num_epochs = 10\n",
        "\n",
        "  for i in range(num_epochs):\n",
        "    total_loss, total_correct, total_num = 0, 0, 0\n",
        "    for batch in train_loader:\n",
        "      #extractign data and labels from batch\n",
        "      X,y = batch[0].to(device).view(batch[0].size()[0],input_dimension), batch[1].to(device)\n",
        "      Y = index_to_one_hot(y, classes=10)\n",
        "      # ----------------------------\n",
        "      # Optimisation -  Your code here\n",
        "\n",
        "\n",
        "      # ----------------------------\n",
        "      # track loss and accuracy after every step\n",
        "      loss_train = CE_loss(X, Y, w)\n",
        "      correct, acc = accuracy(X.mm(w), y)\n",
        "      total_correct += correct\n",
        "      total_loss += loss_train*X.size()[0]\n",
        "      total_num += X.size()[0]\n",
        "\n",
        "    # ----------------------------\n",
        "    # After each epoch track metrics\n",
        "    sgd_train_losses.append(float(total_loss/total_num))\n",
        "    sgd_train_acc.append(float(total_correct/total_num * 100))\n",
        "    val_loss, val_acc = test(w, valid_loader)\n",
        "    sgd_val_losses.append(val_loss)\n",
        "    sgd_val_acc.append(val_acc)\n",
        "    print('train loss: {:.2e} | train acc: {:.2f} | eta: {:.2f}'.format(float(total_loss/total_num), float(total_correct/total_num * 100), eta))\n",
        "    print('val loss: {:.2e} | val acc: {:.2f}'.format(val_loss, val_acc))\n",
        "    print('-'*50)\n",
        "    # ----------------------------\n",
        "    # Adjust the learning rate\n",
        "    eta *= 0.9"
      ],
      "metadata": {
        "id": "0Q_eeCRs8jhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot stuff - reuse this cell for plotting bonus material too.\n",
        "f, axs = plt.subplots(2,2)\n",
        "axs[0,0].plot(np.arange(len(sgd_train_losses)),sgd_train_losses, 'red')\n",
        "axs[0,0].plot(np.arange(len(opt_train_losses)),opt_train_losses, 'blue')\n",
        "axs[0,0].plot(np.arange(len(adam_train_losses)),adam_train_losses, 'black')\n",
        "axs[0,0].set_ylabel('train losses')\n",
        "axs[0,1].plot(np.arange(len(sgd_train_acc)),sgd_train_acc, 'red')\n",
        "axs[0,1].plot(np.arange(len(opt_train_acc)),opt_train_acc, 'blue')\n",
        "axs[0,1].plot(np.arange(len(adam_train_acc)),adam_train_acc, 'black')\n",
        "axs[0,1].set_ylabel('train acc')\n",
        "axs[1,0].plot(np.arange(len(sgd_val_losses)),sgd_val_losses, 'red')\n",
        "axs[1,0].plot(np.arange(len(opt_val_losses)),opt_val_losses, 'blue')\n",
        "axs[1,0].plot(np.arange(len(adam_val_losses)),adam_val_losses, 'black')\n",
        "axs[1,0].set_ylabel('val losses')\n",
        "axs[1,1].plot(np.arange(len(sgd_val_acc)),sgd_val_acc, 'red')\n",
        "axs[1,1].plot(np.arange(len(opt_val_acc)),opt_val_acc, 'blue')\n",
        "axs[1,1].plot(np.arange(len(adam_val_acc)),adam_val_acc, 'black')\n",
        "axs[1,1].set_ylabel('val acc')"
      ],
      "metadata": {
        "id": "tn5Fce4rLnxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can visualise the learnt weights (one for each class) as images by reshapeing into 28x28 grids\n",
        "\n",
        "w_cpu = w.cpu().clone()\n",
        "print(w_cpu.size())\n",
        "\n",
        "f, axs = plt.subplots(1,number_of_classes)\n",
        "\n",
        "for k in range(h):\n",
        "  img = w_cpu[:,k].view(28,28)\n",
        "  axs[k].imshow(img)\n",
        "  axs[k].set_title(k)\n",
        "  axs[k].axis(\"off\")"
      ],
      "metadata": {
        "id": "FwVysM8b8jeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**###Optional###** -  Other optimisers and Adam"
      ],
      "metadata": {
        "id": "rAqPHmFgAG_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**###Optinional### Question 5** implement your some of the optimisers form ex1 and see how they perfrom in this setting? What differences do you notice?"
      ],
      "metadata": {
        "id": "eRS5dzHL_xA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # in this exersize we will not use autograd you will have calucated gradent\n",
        "\n",
        "  # fixing a device to run the codes on\n",
        "  device = torch.device(\"cuda:0\")\n",
        "\n",
        "  w = torch.zeros(input_dimension,number_of_classes).to(device)\n",
        "  m = torch.zeros(input_dimension,number_of_classes).to(device)\n",
        "\n",
        "  eta = 1e-1\n",
        "  num_epochs = 10\n",
        "\n",
        "  for i in range(num_epochs):\n",
        "    total_loss, total_correct, total_num = 0, 0, 0\n",
        "    for batch in train_loader:\n",
        "      #extractign data and labels from batch\n",
        "      X,y = batch[0].to(device).view(batch[0].size()[0],input_dimension), batch[1].to(device)\n",
        "      Y = index_to_one_hot(y, classes=10)\n",
        "      # ----------------------------\n",
        "      # Optimisation -  Your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # ----------------------------\n",
        "      # track loss and accuracy after every step\n",
        "      loss_train = CE_loss(X, Y, w)\n",
        "      correct, acc = accuracy(X.mm(w), y)\n",
        "      total_correct += correct\n",
        "      total_loss += loss_train*X.size()[0]\n",
        "      total_num += X.size()[0]\n",
        "\n",
        "    # ----------------------------\n",
        "    # After each epoch track metrics\n",
        "    opt_train_losses.append(float(total_loss/total_num))\n",
        "    opt_train_acc.append(float(total_correct/total_num * 100))\n",
        "    val_loss, val_acc = test(w, valid_loader)\n",
        "    opt_val_losses.append(val_loss)\n",
        "    opt_val_acc.append(val_acc)\n",
        "    print('train loss: {:.2e} | train acc: {:.2f} | eta: {:.2f}'.format(float(total_loss/total_num), float(total_correct/total_num * 100), eta))\n",
        "    print('val loss: {:.2e} | val acc: {:.2f}'.format(val_loss, val_acc))\n",
        "    print('-'*50)\n",
        "    # ----------------------------\n",
        "    # Adjust the learning rate\n",
        "    eta *= 0.9"
      ],
      "metadata": {
        "id": "uzC1Qjxa_nJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adam** The Adam optimiser is an adeptive gradient method that uses exponial moving averages of previous gradients to estimates a good step size for each parameter. Thus it avoids the need for a learning rate schedule.\n",
        "\n",
        "remember $\\mathbf{m}$, $\\mathbf{v}$ and $\\mathbf{w}$ are vectors.\n",
        "\n",
        "\\begin{align*}\n",
        "  \\hat{\\mathbf{m}_k} &= \\frac{\\mathbf{m}_k}{1-\\beta_1^t},\\hspace{0.1cm} \\mathbf{m}_k \\leftarrow \\beta_1 \\mathbf{m}_{k-1} + (1-\\beta_1)\\nabla \\ell_{z_t}(\\mathbf{w}_t) \\\\\n",
        "  \\hat{\\mathbf{v}}_k &= \\frac{\\mathbf{v}_k}{1-\\beta_2^t},\\hspace{0.1cm} \\mathbf{v}_k \\leftarrow \\beta_2 \\mathbf{v}_{k-1} + (1-\\beta_2)\\nabla \\ell_{z_t}(\\mathbf{w}_t)^2 \\\\\n",
        "  \\mathbf{w}_{k+1} &\\leftarrow \\mathbf{w}_{k} - \\frac{\\eta}{\\sqrt{\\hat{\\mathbf{v}}_k}+\\epsilon} \\hat{\\mathbf{m}}_k \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bjLQporlZdAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**###Optinional### Question 6** implement the ADAM optimiser with $\\beta_1 = 0.9$,$\\beta_2 = 0.999$, $\\eta=0.001$ and run MNIST data set."
      ],
      "metadata": {
        "id": "USJc-v6577eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad(): # in this exersize we will not use autograd you will have calucated gradent\n",
        "\n",
        "  # fixing a device to run the codes on\n",
        "  device = torch.device(\"cuda:0\")\n",
        "\n",
        "  w = torch.zeros(input_dimension,number_of_classes).to(device)\n",
        "  m = torch.zeros(input_dimension,number_of_classes).to(device)\n",
        "\n",
        "  eta = 1e-3\n",
        "  beta_1 = 0.9\n",
        "  beta_2 = 0.999\n",
        "  num_epochs = 10\n",
        "\n",
        "  for i in range(num_epochs):\n",
        "    total_loss, total_correct, total_num = 0, 0, 0\n",
        "    for batch in train_loader:\n",
        "      #extractign data and labels from batch\n",
        "      X,y = batch[0].to(device).view(batch[0].size()[0],input_dimension), batch[1].to(device)\n",
        "      Y = index_to_one_hot(y, classes=10)\n",
        "      # ----------------------------\n",
        "      # Optimisation -  Your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # ----------------------------\n",
        "      # track loss and accuracy after every step\n",
        "      loss_train = CE_loss(X, Y, w)\n",
        "      correct, acc = accuracy(X.mm(w), y)\n",
        "      total_correct += correct\n",
        "      total_loss += loss_train*X.size()[0]\n",
        "      total_num += X.size()[0]\n",
        "\n",
        "    # ----------------------------\n",
        "    # After each epoch track metrics\n",
        "    adam_train_losses.append(float(total_loss/total_num))\n",
        "    adam_train_acc.append(float(total_correct/total_num * 100))\n",
        "    val_loss, val_acc = test(w, valid_loader)\n",
        "    adam_val_losses.append(val_loss)\n",
        "    adam_val_acc.append(val_acc)\n",
        "    print('train loss: {:.2e} | train acc: {:.2f} | eta: {:.2f}'.format(float(total_loss/total_num), float(total_correct/total_num * 100), eta))\n",
        "    print('val loss: {:.2e} | val acc: {:.2f}'.format(val_loss, val_acc))\n",
        "    print('-'*50)"
      ],
      "metadata": {
        "id": "_sPdFp4x-ui9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}