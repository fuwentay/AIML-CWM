{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEGQgkGFPIUE"
      },
      "source": [
        "## CWM: AI/ML with python --- Optimization Notes by Yangchen Pan\n",
        "NOTE: **this exercise is not for submission/grading.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHf9quf-HlJ9"
      },
      "source": [
        "Let's do a coding example by using gradient descent: $\\min_x ||Ax-b||_2^2$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0: function value = 27.227204319242304\n",
            "Iteration 1: function value = 18.51691052250224\n",
            "Iteration 2: function value = 13.99477729311129\n",
            "Iteration 3: function value = 11.62355925849824\n",
            "Iteration 4: function value = 10.357780735963567\n",
            "Iteration 5: function value = 9.660891071119325\n",
            "Iteration 6: function value = 9.257495065052481\n",
            "Iteration 7: function value = 9.006211068389447\n",
            "Iteration 8: function value = 8.834428593196545\n",
            "Iteration 9: function value = 8.704829372798223\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate random data for A and b\n",
        "A = np.random.uniform(0., 1., [10, 5])\n",
        "b = np.random.uniform(1, 4, [10])\n",
        "\n",
        "# Define the function f(x)\n",
        "def f(x):\n",
        "    M = np.dot(A, x) - b\n",
        "    return np.dot(M.T, M)\n",
        "\n",
        "# Define the gradient of f(x)\n",
        "def grad(x):\n",
        "    M = np.dot(A, x) - b\n",
        "    return 2 * np.dot(A.T, M)                                           # Note: Different from what was derived on the whiteboard\n",
        "\n",
        "# Initialize x randomly\n",
        "x = np.random.uniform(1., 2., [5])\n",
        "\n",
        "# Set the learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Perform gradient descent\n",
        "for i in range(10):\n",
        "    x -= learning_rate * grad(x)\n",
        "    print(\"Iteration {}: function value = {}\".format(i, f(x)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To3nhbxjI6u7"
      },
      "source": [
        "Practice: define a function $f(x) = x^\\top A x$, compute its gradient, then do coding like above to test if gradient descent can decrease the function value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Q_Gxv4UcJtfS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24.55094772874336\n",
            "21.962465800900198\n",
            "19.645221758437476\n",
            "17.570727654990144\n",
            "15.713485302370804\n",
            "14.050672415761237\n",
            "12.561861703235135\n",
            "11.22876944143852\n",
            "10.035030442255263\n",
            "8.965996640182272\n",
            "8.008556820931783\n",
            "7.150975272046585\n",
            "6.382747369262238\n",
            "5.694470320846115\n",
            "5.077727478753072\n",
            "4.5249847924592626\n",
            "4.029498130825037\n",
            "3.585230331135502\n",
            "3.1867769542201376\n",
            "2.8292998317354248\n"
          ]
        }
      ],
      "source": [
        "# example of gradient descent\n",
        "\n",
        "import numpy as np\n",
        "A = np.random.uniform(0., 1., [5, 5])\n",
        "x = np.random.uniform(1., 2., [5])\n",
        "\n",
        "# TODO: define the function\n",
        "def f(x):\n",
        "  M = np.dot(A,x)\n",
        "  return np.dot(x.T,M)\n",
        "\n",
        "# TODO: please compute gradient\n",
        "def grad(x):\n",
        "  M = A + A.T\n",
        "  return np.dot(M,x)\n",
        "\n",
        "learning_rate = 0.01\n",
        "# please do 20 gradient steps and print the function value\n",
        "for i in range(20):\n",
        "  # TODO: fill gradient updating rule\n",
        "  x -= learning_rate * grad(x)\n",
        "  print(f(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
