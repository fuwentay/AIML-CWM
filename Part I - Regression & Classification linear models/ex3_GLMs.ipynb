{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD_zp2EXI4Wg"
      },
      "source": [
        "**Question 1**.\n",
        "**Exploring a Real-World Dataset**\n",
        "\n",
        "Let's analyze a dataset with features that include whether it's a weekday, weather conditions (such as temperature and humidity), and more. The target variable is the count of bike rentals. If you are curious about this dataset, you can read more at [UCI Machine Learning Repository - Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset). However, detailed descriptions of the dataset are not necessary to complete this exercise. Our goal is to predict the number of bike rentals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS_FB5xFMDin"
      },
      "source": [
        "1. What do you think is a reasonable distribution to model the target variable?\n",
        "\n",
        "* A: Poisson Distribution since we are trying to count the number of bike rentals within an amount of time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvrJLpYsMI1W"
      },
      "source": [
        "2. By MLE, derive the cost function and write the derivation in latex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* A:\n",
        "$$\n",
        "f(y) = \\frac{\\lambda^{y}e^{-\\lambda}}{y!} \\\\\n",
        "\n",
        "L(\\mu) = \\Pi_{i=1}^{n}{\\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!}} \\\\\n",
        "\n",
        "l(\\mu) = log_{10}(\\mu) = \\sum_{i=1}^{n}log{\\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!}} = \\sum_{i=1}^{n}({y_i{log{\\lambda}} - \\lambda - log{y_i!}}) = \\sum_{i=1}^{n}({y_i{log{\\lambda}} - \\lambda})\n",
        "$$\n",
        "\n",
        "The x_i factorial term above is dropped as it is independent of lambda.\n",
        "\n",
        "Additionally, we will minimise the cost function instead of maximising the likelihood function. Hence,\n",
        "\n",
        "$$\n",
        "l(\\mu) = -\\sum_{i=1}^{n}({y_i{log{\\lambda}} - \\lambda}) \\\\\n",
        "\n",
        "\\lambda_i = e^{x_i^{T}w}\n",
        "$$\n",
        "\n",
        "Therefore,\n",
        "\n",
        "$$\n",
        "l(\\mu) = -\\sum_{i=1}^{n}({y_i{log{e^{x_i^{T}w}}} - e^{x_i^{T}w}}) = -\\sum_{i=1}^{n}({y_i{x_i^{T}w} - e^{x_i^{T}w}})\n",
        "$$\n",
        "\n",
        "Scaling it down by 1/n,\n",
        "\n",
        "$$\n",
        "\\frac{l(\\mu)}{n} = -\\frac{1}{n}\\sum_{i=1}^{n}({y_i{x_i^{T}w} - e^{x_i^{T}w}})\n",
        "$$\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ9Sga4_T7X5"
      },
      "source": [
        "3. Compute the gradient of this objective function $\\min_w -\\frac{1}{n} \\sum_{i=1}^n (y_i x_i^\\top w - e^{x_i^\\top w})$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9AW4fZ7OUXg"
      },
      "source": [
        "Hint: try to compute the gradient of a single data point, then you can sum over all data and scaled it by $1/n$. The summation can be done without using for loop. As you already learned, for loop can be very slow.\n",
        "\n",
        "* A:\n",
        "\n",
        "$$\n",
        "\\nabla{\\frac{l(\\mu)}{n}} = \\frac{d}{dw}{(-\\frac{1}{n}\\sum_{i=1}^{n}({y_i{x_i^{T}w} - e^{x_i^{T}w}}))} = -\\frac{1}{n} \\frac{d}{dw}{\\sum_{i=1}^{n}({y_i{x_i^{T}w} - e^{x_i^{T}w}})} = -\\frac{1}{n}{\\sum_{i=1}^{n}{(y_i{x_i}-e^{x_i^{T}{w}}x_i)}} = -\\frac{1}{n}{\\sum_{i=1}^{n}{x_i{(y_i-e^{x_i^{T}w})}}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unujbn22OWtd"
      },
      "source": [
        "4. Optimizing the above objective function by gradient descent. The goal is to make reasonable predictions of bike rentals. Part of the code is already given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9khQqIOKObrQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4999, 115)\n",
            "[ 50.   5. 225.  28. 102. 313.   4. 332. 576.  92.]\n"
          ]
        }
      ],
      "source": [
        "# The original dataset is large, below I provide a cleaned subset.\n",
        "# NO need to edit this block\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(0)\n",
        "fileloc = 'https://raw.githubusercontent.com/yannickycpan/oxford-engs-AIML-cwm/main/subsetbikeshare.txt'\n",
        "dataset = pd.read_csv(fileloc)\n",
        "dataset = dataset.values\n",
        "np.random.shuffle(dataset)\n",
        "print(dataset.shape)\n",
        "np.random.shuffle(dataset)\n",
        "# I already put the target variable the last column\n",
        "Y = dataset[:, -1]\n",
        "X = dataset[:, :-1]\n",
        "X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
        "\n",
        "valn = int(0.1*X.shape[0])\n",
        "Yval = Y[:valn]\n",
        "Xval = X[:valn]\n",
        "\n",
        "X=X[valn:]\n",
        "Y=Y[valn:]\n",
        "\n",
        "print(Y[:10])\n",
        "\n",
        "def rmse(ytrue, ypred):\n",
        "  return np.sqrt(np.mean((ytrue-ypred)**2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4500, 115) (4500,)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape, Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3jBBWlZ1PG0q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " validation err is ::  253.05831260355257\n",
            " train err is ::  258.14321143327703\n",
            " validation err is ::  101.0602113436416\n",
            " train err is ::  98.45896719849128\n",
            " validation err is ::  94.10932692441442\n",
            " train err is ::  90.89283554975972\n",
            " validation err is ::  91.5415732336252\n",
            " train err is ::  88.30430024731653\n",
            " validation err is ::  90.37403498987621\n",
            " train err is ::  87.20876925256977\n",
            " validation err is ::  89.77517802445003\n",
            " train err is ::  86.65421511784442\n",
            " validation err is ::  89.45198825258652\n",
            " train err is ::  86.33751629982505\n",
            " validation err is ::  89.27361891563514\n",
            " train err is ::  86.1400895073618\n",
            " validation err is ::  89.17457770714961\n",
            " train err is ::  86.00837906547643\n",
            " validation err is ::  89.12030621738121\n",
            " train err is ::  85.91556327300144\n",
            " validation err is ::  89.09192669761997\n",
            " train err is ::  85.84706964988898\n",
            " validation err is ::  89.07882076176318\n",
            " train err is ::  85.79446011443952\n",
            " validation err is ::  89.07485451818228\n",
            " train err is ::  85.75260409752687\n",
            " validation err is ::  89.07639877093693\n",
            " train err is ::  85.71826235223917\n",
            " validation err is ::  89.08126146745361\n",
            " train err is ::  85.68932722866619\n",
            " validation err is ::  89.08809602648516\n",
            " train err is ::  85.66439189847436\n",
            " validation err is ::  89.09606447132421\n",
            " train err is ::  85.64249533819873\n",
            " validation err is ::  89.10464071152276\n",
            " train err is ::  85.62296632075692\n",
            " validation err is ::  89.11349300239763\n",
            " train err is ::  85.60532555767283\n",
            " validation err is ::  89.12241229096905\n",
            " train err is ::  85.58922310507907\n",
            " validation err is ::  89.131267779494\n",
            " train err is ::  85.57439767865255\n",
            " validation err is ::  89.13997896727697\n",
            " train err is ::  85.56064983484664\n",
            " validation err is ::  89.14849784699778\n",
            " train err is ::  85.54782405773958\n",
            " validation err is ::  89.15679745396591\n",
            " train err is ::  85.5357966358547\n",
            " validation err is ::  89.16486444143942\n",
            " train err is ::  85.52446734450685\n",
            " validation err is ::  89.17269423553442\n",
            " train err is ::  85.51375365535564\n",
            " validation err is ::  89.18028785845216\n",
            " train err is ::  85.50358664173625\n",
            " validation err is ::  89.18764983925004\n",
            " train err is ::  85.49390803425267\n",
            " validation err is ::  89.19478683827585\n",
            " train err is ::  85.48466806569846\n",
            " validation err is ::  89.2017067424445\n",
            " train err is ::  85.4758238644741\n"
          ]
        }
      ],
      "source": [
        "def grad(w):\n",
        "  # TODO: complete the code here to compute gradient\n",
        "  yhat = np.exp(X@w)\n",
        "  return -X.T@(Y-yhat)/len(Y)\n",
        "\n",
        "lr = 0.001  # learning rate\n",
        "\n",
        "w = np.zeros(X.shape[1])\n",
        "for i in range(3000):\n",
        "  # TODO: complete the gradient updating rule\n",
        "  w -= lr * grad(w)\n",
        "\n",
        "  if i%100==0: # print out the root mean squared error every 100 steps\n",
        "    # TODO: please replace the question marks below\n",
        "    Yvalpred = np.exp(Xval@w)\n",
        "    Ypred = np.exp(X@w)\n",
        "    print(' validation err is :: ', rmse(Yval, Yvalpred))\n",
        "    print(' train err is :: ', rmse(Y, Ypred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9t4MpsJsBJp"
      },
      "source": [
        "5. Test the linear regression model we learned previously on this dataset. How does it compare with the algorithm above? You should find that by incorporating appropriate prior knowledge—specifically (i.e., recognizing that the target variable represents counts—into the model building process), we can achieve better performance.\n",
        "\n",
        "* A: The smaller validation and training error shows that the result improves after adding prior knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Linear regression:\n",
        "\n",
        "$$\n",
        "L(w) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - x_i^\\top w)^2 \\\\\n",
        "\n",
        "\\nabla_w L(w) = -\\frac{1}{n} \\sum_{i=1}^{n} (y_i - x_i^\\top w) x_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vYA3kln4q_Jm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "validation err is ::  247.978675572676\n",
            "train err is ::  253.1243774289587\n",
            "validation err is ::  148.1711421747072\n",
            "train err is ::  155.98531070740694\n",
            "validation err is ::  139.79645597569143\n",
            "train err is ::  146.8628697852999\n",
            "validation err is ::  133.29832790051137\n",
            "train err is ::  139.65442832386384\n",
            "validation err is ::  128.14023471220395\n",
            "train err is ::  133.84976217890463\n",
            "validation err is ::  123.9960266660276\n",
            "train err is ::  129.11878096894543\n",
            "validation err is ::  120.63525217105324\n",
            "train err is ::  125.22759900111316\n",
            "validation err is ::  117.88695035325014\n",
            "train err is ::  122.00219515995211\n",
            "validation err is ::  115.62074161070257\n",
            "train err is ::  119.30912222785277\n",
            "validation err is ::  113.73576652366687\n",
            "train err is ::  117.0442615934371\n",
            "validation err is ::  112.15348360963696\n",
            "train err is ::  115.12560143437135\n",
            "validation err is ::  110.81256431563992\n",
            "train err is ::  113.48820367922805\n",
            "validation err is ::  109.66507215056598\n",
            "train err is ::  112.08048768927002\n",
            "validation err is ::  108.67351829527186\n",
            "train err is ::  110.86138748951066\n",
            "validation err is ::  107.80856309521185\n",
            "train err is ::  109.79813530914565\n",
            "validation err is ::  107.04721460028671\n",
            "train err is ::  108.86451784561542\n",
            "validation err is ::  106.37141727129998\n",
            "train err is ::  108.03949988810771\n",
            "validation err is ::  105.76694891658006\n",
            "train err is ::  107.30613749395832\n",
            "validation err is ::  105.22256106424724\n",
            "train err is ::  106.65072061761518\n",
            "validation err is ::  104.72931099705548\n",
            "train err is ::  106.06209769086034\n",
            "validation err is ::  104.28004407706615\n",
            "train err is ::  105.53114426326911\n",
            "validation err is ::  103.86899343744754\n",
            "train err is ::  105.0503454253877\n",
            "validation err is ::  103.49147098815816\n",
            "train err is ::  104.61346786666726\n",
            "validation err is ::  103.14362923288564\n",
            "train err is ::  104.21530237408592\n",
            "validation err is ::  102.82227784384501\n",
            "train err is ::  103.85146157262571\n",
            "validation err is ::  102.52474247950525\n",
            "train err is ::  103.51822091611213\n",
            "validation err is ::  102.24875612488647\n",
            "train err is ::  103.21239349822395\n",
            "validation err is ::  101.99237542814556\n",
            "train err is ::  102.93123128844339\n",
            "validation err is ::  101.75391622136237\n",
            "train err is ::  102.67234700707556\n",
            "validation err is ::  101.531903747327\n",
            "train err is ::  102.43365212116625\n"
          ]
        }
      ],
      "source": [
        "# TODO: compute the gradient of the linear regression objective\n",
        "def grad(lrw):\n",
        "  y_hat = X.dot(lrw)\n",
        "  return -X.T.dot(Y-y_hat)/len(Y)\n",
        "\n",
        "# feel free to change the learning rate if you want\n",
        "lrw = np.zeros(X.shape[1])\n",
        "for i in range(3000):\n",
        "  lrw = lrw - 0.01 * grad(lrw)\n",
        "\n",
        "  if i%100==0:\n",
        "    # TODO: print out the root mean squared error every 100 steps\n",
        "    # please replace the question marks below\n",
        "    y_val_pred = Xval.dot(lrw)\n",
        "    y_train_pred = X.dot(lrw)\n",
        "    print('validation err is :: ', rmse(Yval, y_val_pred))\n",
        "    print('train err is :: ', rmse(Y, y_train_pred))\n",
        "\n",
        "# (optional) TODO: you can also use closed-form solution\n",
        "# and directly print the validation and training error below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfjMPXlBQME9"
      },
      "source": [
        "**Question 2**. Assume that you know some feature columns (i.e. the column indexes) are highly inaccurate on the above bikeshare dataset. Could you design a new regularization method to help improve the solution? Note that you are NOT required to implement your method, but you should write the math expression of the regularization you designed in an implementable way and provide some nice/reasonable explanations to your design. If you want to do coding test for your new regularization, feel free to add a coding cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noVOHYrTHoYR"
      },
      "source": [
        "We can define a new regularization term that imposes a stronger penalty on the weights corresponding to the inaccurate features. One way to do this is by introducing different regularization strengths for the accurate and inaccurate features.\n",
        "\n",
        "The overall loss function with this new regularization term can be written as:\n",
        "$\n",
        "L(w)=\\frac{1}{n}{\\sum_{i=1}^{n}{l(y_i,\\hat y_i)+\\lambda R(w)}}\n",
        "$\n",
        "\n",
        "where, l is the loss function and lambda is the regularisation parameter.\n",
        "\n",
        "By imposing a stronger penalty on the inaccurate features, we effectively reduce their impact on the model.\n",
        "Additionally, the method allows for different levels of regularization, which can be tuned based on the degree of inaccuracy known for each feature. This flexibility can be useful in practice where some features are more inaccurate than others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTOlwexOCs74"
      },
      "source": [
        "**Question 3 Two subquestions about the derivation of the cost function for multiclass classification problems**. `\n",
        "\n",
        "Modeling multiclass data. Logistic regression we learned above is good for binary class. What if we have $m>2$ classes? We will assume multinomial distribution. You will finally get the very popular softmax cross entropy loss. In the rest of this question, a vector is a column vector unless otherwise specified.\n",
        "\n",
        "To deal with multiclass classification problem, it is common to assume that $p(y|x)$ is multinomial distribution (some places call categorical distribution as it is a special case of multinomial distribution), and apply MLE. Given a sample $x, y$, we can write (a slight notation abuse: $y_i \\in \\{0,1\\}$ here means the ith component in the one-hot vector y which is m-dimensional). The conditional distribution of a data point $x, y$ is:\n",
        "\\begin{align}\n",
        "p(y|x) &= \\frac{1}{y_1 ! ... y_m !} p(y_1=1|x)^{y_1}...p(y_m=1|x)^{y_m}\n",
        "\\end{align}\n",
        "We can parameterize those $m$ probabilities by a softmax function: $\\sigma(x^\\top W), W \\in \\mathbb{R}^{d\\times m}$. The softmax function is defined as: $\\sigma :\\mathbb {R} ^{m}\\to (0,1)^{m}, $\n",
        "$\\sigma (z)_{i}={\\frac {e^{z_{i}}}{\\sum _{j=1}^{m}e^{z_{j}}}}\\ \\ {\\text{ for }}i=1,\\dotsc ,m{\\text{ and }} z =(z_{1},\\dotsc ,z_{m})\\in \\mathbb {R} ^{m}.$\n",
        "\n",
        "Note:\n",
        "- one-hot vector is the one with a single entry to be one and other $m-1$ entries are zero. It is common to convert an integer label to one-hot vector in classification problems. For integer $i$, the ith entry is 1 otherwise zero. For example, for integer zero, the one-hot vector is $(1, 0, ..., 0)$. Here, we have m classes (m possible labels from 0, 1, 2, ..., m-1). As a result, the one-hot vector is m-dimensional.\n",
        "\n",
        "- in this question we use superscript to denote a sample, subscript to denote a component in a vector.\n",
        "\n",
        "- $x^{(i)}$ is a column vector $d-by-1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYQ0HiQXkzXd"
      },
      "source": [
        "1. Based on the above hints, please complete the derivation to get the cost function below.\n",
        "\n",
        "Finally, we arrive the following cost function:\n",
        "$\n",
        "\\min_w \\frac{1}{n} \\sum_{i=1}^n \\log(\\mathbf{1}^\\top \\exp{(x^{(i) \\top} W)}^\\top ) - x^{(i)\\top} W y^{(i)}\n",
        "$  \n",
        "where I use superscripe to denote the $i$th training example and $\\mathbf{1}$ is a $m$-dimensional column vector with all ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* A:\n",
        "\n",
        "Likelihood of y given x:\n",
        "\n",
        "\\begin{align*}\n",
        "\\\n",
        "p(y|x) = \\prod_{j=1}^m \\left( \\frac{\\exp(x^\\top W_j)}{\\sum_{k=1}^m \\exp(x^\\top W_k)} \\right)^{y_j}\n",
        "\\\n",
        "\\end{align*}\n",
        "\n",
        "Taking its log:\n",
        "\n",
        "\\begin{align*}\n",
        "\\\n",
        "\\log p(y|x) = \\log \\left( \\prod_{j=1}^m \\left( \\frac{\\exp(x^\\top W_j)}{\\sum_{k=1}^m \\exp(x^\\top W_k)} \\right)^{y_j} \\right) = \\sum_{j=1}^m y_j \\log \\left( \\frac{\\exp(x^\\top W_j)}{\\sum_{k=1}^m \\exp(x^\\top W_k)} \\right) = \\sum_{j=1}^m y_j \\left( x^\\top W_j - \\log \\left( \\sum_{k=1}^m \\exp(x^\\top W_k) \\right) \\right)\n",
        "\\\n",
        "\\end{align*}\n",
        "\n",
        "Dividing by n and converting element-wise to vector/matrix, cost function:\n",
        "\n",
        "\\begin{align*}\n",
        "\\\n",
        "\\\\min_W \\frac{1}{n} \\sum_{i=1}^n \\left( \\log \\left( \\mathbf{1}^\\top \\exp \\left( x^{(i) \\top} W \\right) \\right) - x^{(i) \\top} W y^{(i)} \\right)\n",
        "\\\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALrwBIZdj1_y"
      },
      "source": [
        "2. **[optional]** You may also see a cost function for multiclassification looks like $-\\frac{1}{n} \\sum_{i=1}^n y^{(i)\\top} \\log(softmax(x^{(i) \\top} W)^\\top)$. Is it equivalent to the above cost function? Please prove your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgqXSBumVevZ"
      },
      "source": [
        "**Question 4 [optional]**\n",
        "\n",
        "Based on your understanding of MLE, develop a binary classifier that models the positive class as:\n",
        "\n",
        "$\n",
        "p(y=1|x,w)=\\frac{1}{2}(1 + \\frac{w^\\top x}{\\sqrt{1+(w^\\top x)^2}})\n",
        "$\n",
        "\n",
        "You need to write the log-likelihood function and compute the gradient."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
