{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**.\n",
        "**Exploring a Real-World Dataset**\n",
        "\n",
        "Let's analyze a dataset with features that include whether it's a weekday, weather conditions (such as temperature and humidity), and more. The target variable is the count of bike rentals. If you are curious about this dataset, you can read more at [UCI Machine Learning Repository - Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset). However, detailed descriptions of the dataset are not necessary to complete this exercise. Our goal is to predict the number of bike rentals."
      ],
      "metadata": {
        "id": "fD_zp2EXI4Wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What do you think is a reasonable distribution to model the target variable?"
      ],
      "metadata": {
        "id": "SS_FB5xFMDin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. By MLE, derive the cost function and write the derivation in latex."
      ],
      "metadata": {
        "id": "EvrJLpYsMI1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Compute the gradient of this objective function $\\min_w -\\frac{1}{n} \\sum_{i=1}^n (y_i x_i^\\top w - e^{x_i^\\top w})$.\n"
      ],
      "metadata": {
        "id": "KZ9Sga4_T7X5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hint: try to compute the gradient of a single data point, then you can sum over all data and scaled it by $1/n$. The summation can be done without using for loop. As you already learned, for loop can be very slow."
      ],
      "metadata": {
        "id": "J9AW4fZ7OUXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Optimizing the above objective function by gradient descent. The goal is to make reasonable predictions of bike rentals. Part of the code is already given."
      ],
      "metadata": {
        "id": "Unujbn22OWtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The original dataset is large, below I provide a cleaned subset.\n",
        "# NO need to edit this block\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(0)\n",
        "fileloc = 'https://raw.githubusercontent.com/yannickycpan/oxford-engs-AIML-cwm/main/subsetbikeshare.txt'\n",
        "dataset = pd.read_csv(fileloc)\n",
        "dataset = dataset.values\n",
        "np.random.shuffle(dataset)\n",
        "print(dataset.shape)\n",
        "np.random.shuffle(dataset)\n",
        "# I already put the target variable the last column\n",
        "Y = dataset[:, -1]\n",
        "X = dataset[:, :-1]\n",
        "X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
        "\n",
        "valn = int(0.1*X.shape[0])\n",
        "Yval = Y[:valn]\n",
        "Xval = X[:valn]\n",
        "\n",
        "X=X[valn:]\n",
        "Y=Y[valn:]\n",
        "\n",
        "print(Y[:10])\n",
        "\n",
        "def rmse(ytrue, ypred):\n",
        "  return np.sqrt(np.mean((ytrue-ypred)**2))"
      ],
      "metadata": {
        "id": "9khQqIOKObrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad(w):\n",
        "  # TODO: complete the code here to compute gradient\n",
        "  return\n",
        "\n",
        "w = np.zeros(X.shape[1])\n",
        "for i in range(3000):\n",
        "  # TODO: complete the gradient updating rule\n",
        "  w =\n",
        "\n",
        "  if i%100==0: # print out the root mean squared error every 100 steps\n",
        "    # TODO: please replace the question marks below\n",
        "    print(' validation err is :: ', rmse(Yval, ???))\n",
        "    print(' train err is :: ', rmse(Y, ???))"
      ],
      "metadata": {
        "id": "3jBBWlZ1PG0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Test the linear regression model we learned previously on this dataset. How does it compare with the algorithm above? You should find that by incorporating appropriate prior knowledge—specifically (i.e., recognizing that the target variable represents counts—into the model building process), we can achieve better performance."
      ],
      "metadata": {
        "id": "e9t4MpsJsBJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: compute the gradient of the linear regression objective\n",
        "def grad(lrw):\n",
        "  return\n",
        "\n",
        "# feel free to change the learning rate if you want\n",
        "lrw = np.zeros(X.shape[1])\n",
        "for i in range(3000):\n",
        "  lrw = lrw - 0.01 * grad(lrw)\n",
        "\n",
        "  if i%100==0:\n",
        "    # TODO: print out the root mean squared error every 100 steps\n",
        "    # please replace the question marks below\n",
        "    print(' validation err is :: ', rmse(Yval, ???))\n",
        "    print(' train err is :: ', rmse(Y, ???))\n",
        "\n",
        "# (optional) TODO: you can also use closed-form solution\n",
        "# and directly print the validation and training error below:"
      ],
      "metadata": {
        "id": "vYA3kln4q_Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**. Assume that you know some feature columns (i.e. the column indexes) are highly inaccurate on the above bikeshare dataset. Could you design a new regularization method to help improve the solution? Note that you are NOT required to implement your method, but you should write the math expression of the regularization you designed in an implementable way and provide some nice/reasonable explanations to your design. If you want to do coding test for your new regularization, feel free to add a coding cell below."
      ],
      "metadata": {
        "id": "BfjMPXlBQME9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "noVOHYrTHoYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3 Two subquestions about the derivation of the cost function for multiclass classification problems**. `\n",
        "\n",
        "Modeling multiclass data. Logistic regression we learned above is good for binary class. What if we have $m>2$ classes? We will assume multinomial distribution. You will finally get the very popular softmax cross entropy loss. In the rest of this question, a vector is a column vector unless otherwise specified.\n",
        "\n",
        "To deal with multiclass classification problem, it is common to assume that $p(y|x)$ is multinomial distribution (some places call categorical distribution as it is a special case of multinomial distribution), and apply MLE. Given a sample $x, y$, we can write (a slight notation abuse: $y_i \\in \\{0,1\\}$ here means the ith component in the one-hot vector y which is m-dimensional). The conditional distribution of a data point $x, y$ is:\n",
        "\\begin{align}\n",
        "p(y|x) &= \\frac{1}{y_1 ! ... y_m !} p(y_1=1|x)^{y_1}...p(y_m=1|x)^{y_m}\n",
        "\\end{align}\n",
        "We can parameterize those $m$ probabilities by a softmax function: $\\sigma(x^\\top W), W \\in \\mathbb{R}^{d\\times m}$. The softmax function is defined as: $\\sigma :\\mathbb {R} ^{m}\\to (0,1)^{m}, $\n",
        "$\\sigma (z)_{i}={\\frac {e^{z_{i}}}{\\sum _{j=1}^{m}e^{z_{j}}}}\\ \\ {\\text{ for }}i=1,\\dotsc ,m{\\text{ and }} z =(z_{1},\\dotsc ,z_{m})\\in \\mathbb {R} ^{m}.$\n",
        "\n",
        "Note:\n",
        "- one-hot vector is the one with a single entry to be one and other $m-1$ entries are zero. It is common to convert an integer label to one-hot vector in classification problems. For integer $i$, the ith entry is 1 otherwise zero. For example, for integer zero, the one-hot vector is $(1, 0, ..., 0)$. Here, we have m classes (m possible labels from 0, 1, 2, ..., m-1). As a result, the one-hot vector is m-dimensional.\n",
        "\n",
        "- in this question we use superscript to denote a sample, subscript to denote a component in a vector.\n",
        "\n",
        "- $x^{(i)}$ is a column vector $d-by-1$."
      ],
      "metadata": {
        "id": "sTOlwexOCs74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Based on the above hints, please complete the derivation to get the cost function below.\n",
        "\n",
        "Finally, we arrive the following cost function:\n",
        "$\n",
        "\\min_w \\frac{1}{n} \\sum_{i=1}^n \\log(\\mathbf{1}^\\top \\exp{(x^{(i) \\top} W)}^\\top ) - x^{(i)\\top} W y^{(i)}\n",
        "$  \n",
        "where I use superscripe to denote the $i$th training example and $\\mathbf{1}$ is a $m$-dimensional column vector with all ones."
      ],
      "metadata": {
        "id": "nYQ0HiQXkzXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **[optional]** You may also see a cost function for multiclassification looks like $-\\frac{1}{n} \\sum_{i=1}^n y^{(i)\\top} \\log(softmax(x^{(i) \\top} W)^\\top)$. Is it equivalent to the above cost function? Please prove your answer."
      ],
      "metadata": {
        "id": "ALrwBIZdj1_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4 [optional]**\n",
        "\n",
        "Based on your understanding of MLE, develop a binary classifier that models the positive class as:\n",
        "\n",
        "$\n",
        "p(y=1|x,w)=\\frac{1}{2}(1 + \\frac{w^\\top x}{\\sqrt{1+(w^\\top x)^2}})\n",
        "$\n",
        "\n",
        "You need to write the log-likelihood function and compute the gradient."
      ],
      "metadata": {
        "id": "mgqXSBumVevZ"
      }
    }
  ]
}