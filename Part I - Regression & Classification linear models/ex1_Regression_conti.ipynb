{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4** Thinking geometrically.\n",
        "\n",
        "Below, a dataset and its linear regression solution are provided. Let $c_1, c_2, c_3, c_4,$ and $c_5$ be the five columns in the feature matrix. Randomly sample five real values $u_1, u_2, u_3, u_4, u_5$ from a uniform distribution $(0, 1)$ and generate a new vector by using these values to linearly combine the aforementioned columns: $c = \\sum_{i=1}^5 u_i c_i$. Calculate the inner product $c^\\top (y-\\hat{y})$, where $y$ is the training target vector and $\\hat{y}$ is the predicted target vector. Why do you get such result?\n",
        "\n",
        "Additionally, what is the value of $\\hat{y}^\\top (y-\\hat{y})$? Explain why this value is what it is.\n",
        "\n",
        "You may choose to verify your answer through coding. Note that very small values (e.g., $\\leq 1e-10$) can be treated as zeros.\n"
      ],
      "metadata": {
        "id": "MRhL-t8m5Zm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "pi=math.pi\n",
        "# generate 10 numbers from -1 to 1 with equal stepsize\n",
        "x=np.linspace(-1,1,100)\n",
        "\n",
        "# generate training target (noise contaminated!)\n",
        "y=np.sin(2*pi*.5*x)+0.4*np.random.randn(x.size)\n",
        "\n",
        "M = 5\n",
        "basis = np.arange(M+1)\n",
        "X = x[:, np.newaxis]**basis[np.newaxis, :]\n",
        "\n",
        "w = np.linalg.solve(X.T@X, X.T@y)\n",
        "yhat = X.dot(w)\n",
        "r = y-yhat"
      ],
      "metadata": {
        "id": "f3PGBAlzpa-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5** Investigating Solution Sensitivity.\n",
        "\n",
        "1. Determine the solution using the Singular Value Decomposition (SVD) of the data matrix and the training target $y$. You must **not** use any pre-existing API for this task. Ensure that your computations avoid any matrix-matrix products. Your final solution should match the one obtained using `np.linalg.lstsq()`.\n",
        "\n",
        "**Note:** The order of computations is crucial in matrix calculations as it can significantly affect computational costs."
      ],
      "metadata": {
        "id": "3TZVBdbx81_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "pi=math.pi\n",
        "# generate 10 numbers from -1 to 1 with equal stepsize\n",
        "x=np.linspace(-1,1,50)\n",
        "# generate training target (noise contaminated!)\n",
        "y=np.sin(2*pi*.5*x)+0.4*np.random.randn(x.size)\n",
        "\n",
        "M = 10\n",
        "basis = np.arange(M+1)\n",
        "X = x[:, np.newaxis]**basis[np.newaxis, :]\n",
        "\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html\n",
        "U, S, VT = np.linalg.svd(X)\n",
        "print(U.shape, S.shape, VT.shape)\n",
        "\n",
        "## TODO: Please fill the function to compute the linear regression solution\n",
        "## on dataset (X, y) which is already provided\n",
        "## the computation should NOT involve matrix-matrix product,\n",
        "## matrix-vector product is fine\n",
        "## and should NOT use any solution API like np.linalg.solve, etc.\n",
        "def getbysvd():\n",
        "  return theta\n",
        "\n",
        "theta = getthetabysvd()\n",
        "thetabylstsq = np.linalg.lstsq(X, y)[0]\n",
        "print(\"if your computation is correct, here should print out true :: \", np.allclose(theta, thetabylstsq))\n"
      ],
      "metadata": {
        "id": "HKMMq5EuMdXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. There are various definitions of sensitivity. In this exercise, 'sensitivity' refers to the extent to which a model's output is perturbed when noise is added to the input. How does sensitivity change as regularization increases? Please visualize the change in sensitivity as regularization increases using a separate figure."
      ],
      "metadata": {
        "id": "8iJazT4Jihvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Without using an additional regularization term**, can you identify another method to reduce sensitivity? Hint: Review the documentation for the `lstsq` function (doc: [numpy.linalg.lstsq](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq)). Please empirically test if this method works and explain why.\n",
        "\n",
        "4. **Relationship Between Sensitivity and Overfitting**:\n",
        "   - Do you think reducing sensitivity can always reduce overfitting?\n",
        "   - Conversely, do you think reducing overfitting can always reduce sensitivity?\n",
        "   \n",
        "   Please share your thoughts and reasoning on these questions."
      ],
      "metadata": {
        "id": "HLuunzzoQgzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "pi=math.pi\n",
        "# generate 10 numbers from -1 to 1 with equal stepsize\n",
        "x=np.linspace(-1,1,10)\n",
        "\n",
        "# generate training target (noise contaminated!)\n",
        "y=np.sin(2*pi*.5*x)+0.4*np.random.randn(x.size)\n",
        "\n",
        "# define a validation set\n",
        "xv=np.linspace(-1,1,20)\n",
        "yv=np.sin(2*pi*.5*xv)+0.4*np.random.randn(xv.size)"
      ],
      "metadata": {
        "id": "cOSxUannivhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sensi(theta, xtilde):\n",
        "  yhat_orig = xtilde@theta\n",
        "  yhat_p = (xtilde+np.random.normal(0., 0.2, xtilde.shape))@theta\n",
        "  return np.linalg.norm(yhat_p-yhat_orig)/np.linalg.norm(yhat_orig)\n",
        "\n",
        "np.random.seed(0)\n",
        "error_val= []\n",
        "error_train = []\n",
        "sensi = []\n",
        "\n",
        "# try increasing number of basis\n",
        "reglist = [0.0, 0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
        "for reg in reglist:\n",
        "  plt.figure()\n",
        "\n",
        "  # always show the true function\n",
        "  # plot the true function\n",
        "  plt.plot(np.linspace(-1,1,50), np.sin(2*pi*.5*np.linspace(-1,1,50)), 'black', label='true')\n",
        "\n",
        "  M = 30\n",
        "  basis = np.arange(M+1)\n",
        "  X = x[:, np.newaxis]**basis[np.newaxis, :]\n",
        "  print(X.shape)\n",
        "\n",
        "  theta = np.linalg.solve(X.T@X + reg*np.eye(X.shape[1]), X.T@y)\n",
        "  yhat = X@theta\n",
        "  # plot the fitted function\n",
        "  plt.plot(x,yhat, label='fitted')\n",
        "\n",
        "  # plot the train and validation data\n",
        "  plt.plot(x,y,'ro', label='train')\n",
        "\n",
        "  # show labels\n",
        "  plt.legend()\n",
        "\n",
        "  # compute val error and train error\n",
        "  Xv = xv[:, np.newaxis]**basis[np.newaxis, :]\n",
        "  yhat_val = Xv@theta\n",
        "  error = np.sum((yv - yhat_val)**2)\n",
        "  error_t = np.sum((y - yhat)**2)\n",
        "\n",
        "  error_val.append(error)\n",
        "  error_train.append(error_t)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "# TODO: add a plot to visualize how validation error changes as we increase basis\n",
        "\n",
        "plt.plot(error_val, label='val-err')\n",
        "plt.plot(error_train, label='train-err')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "A3Ii7hPPTaKn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}